# Example Workflow - Data Processing Pipeline
# This workflow demonstrates data processing with error handling and parallel execution

name: data_processing_pipeline
version: "1.0"
description: "Automated data processing pipeline with validation, transformation, and analysis"

variables:
  - name: input_file_path
    type: string
    required: true
    description: "Path to input data file"
  
  - name: output_directory
    type: string
    required: true
    description: "Directory for processed output files"
  
  - name: processing_config
    type: object
    default: {}
    description: "Configuration parameters for processing"
  
  - name: validation_results
    type: object
    description: "Data validation results"
  
  - name: processed_data
    type: object
    description: "Transformed and processed data"

timeout: 3600  # 1 hour
max_parallel_tasks: 5

tasks:
  # Step 1: Validate input data
  - id: validate_input
    name: "Validate Input Data"
    type: agent
    agent: "data_validation_agent"
    parameters:
      file_path: "${input_file_path}"
      validation_rules: "${processing_config.validation_rules}"
    outputs:
      validation_results: validation_results
      is_valid: is_data_valid
      record_count: input_record_count
    retry:
      max_attempts: 2
      delay_seconds: 5
  
  # Step 2: Early exit if data is invalid
  - id: validation_check
    name: "Check Validation Results"
    type: condition
    depends_on: ["validate_input"]
    condition:
      variable: is_data_valid
      operator: equals
      value: false
    # This would typically trigger an alert/notification workflow
  
  # Step 3: Data preprocessing (parallel tasks)
  - id: data_preprocessing
    name: "Data Preprocessing"
    type: parallel
    depends_on: ["validate_input"]
    condition:
      variable: is_data_valid
      operator: equals
      value: true
    tasks:
      # Clean data
      - id: clean_data
        name: "Clean Data"
        type: agent
        agent: "data_cleaning_agent"
        parameters:
          input_file: "${input_file_path}"
          cleaning_config: "${processing_config.cleaning_rules}"
        outputs:
          cleaned_data: cleaned_data_path
      
      # Extract features
      - id: feature_extraction
        name: "Extract Features"
        type: agent
        agent: "feature_extraction_agent"
        parameters:
          input_file: "${input_file_path}"
          feature_config: "${processing_config.feature_rules}"
        outputs:
          features_data: features_data_path
      
      # Generate metadata
      - id: generate_metadata
        name: "Generate Metadata"
        type: mcp_tool
        tool: "generate_data_metadata"
        parameters:
          input_file: "${input_file_path}"
          record_count: "${input_record_count}"
        outputs:
          metadata: data_metadata
  
  # Step 4: Transform data
  - id: transform_data
    name: "Transform Data"
    type: agent
    agent: "data_transformation_agent"
    depends_on: ["data_preprocessing"]
    parameters:
      cleaned_data: "${cleaned_data_path}"
      features_data: "${features_data_path}"
      transformation_config: "${processing_config.transformation_rules}"
    outputs:
      transformed_data: transformed_data_path
      transformation_stats: transformation_stats
  
  # Step 5: Quality checks
  - id: quality_assurance
    name: "Data Quality Assurance"
    type: agent
    agent: "data_quality_agent"
    depends_on: ["transform_data"]
    parameters:
      data_file: "${transformed_data_path}"
      quality_thresholds: "${processing_config.quality_thresholds}"
      original_record_count: "${input_record_count}"
    outputs:
      quality_score: data_quality_score
      quality_issues: quality_issues
      passed_qa: passed_quality_check
  
  # Step 6: Conditional processing based on quality
  - id: handle_quality_issues
    name: "Handle Quality Issues"
    type: agent
    agent: "quality_remediation_agent"
    depends_on: ["quality_assurance"]
    condition:
      variable: passed_quality_check
      operator: equals
      value: false
    parameters:
      quality_issues: "${quality_issues}"
      data_file: "${transformed_data_path}"
    outputs:
      remediated_data: remediated_data_path
      remediation_actions: remediation_actions
  
  # Step 7: Final data analysis (parallel analysis tasks)
  - id: data_analysis
    name: "Data Analysis"
    type: parallel
    depends_on: ["quality_assurance"]
    condition:
      variable: passed_quality_check
      operator: equals
      value: true
    tasks:
      # Statistical analysis
      - id: statistical_analysis
        name: "Statistical Analysis"
        type: agent
        agent: "statistics_agent"
        parameters:
          data_file: "${transformed_data_path}"
          analysis_config: "${processing_config.stats_config}"
        outputs:
          statistics: statistical_results
      
      # Generate visualizations
      - id: create_visualizations
        name: "Create Visualizations"
        type: agent
        agent: "visualization_agent"
        parameters:
          data_file: "${transformed_data_path}"
          chart_config: "${processing_config.chart_config}"
          output_dir: "${output_directory}"
        outputs:
          visualization_paths: chart_files
      
      # Anomaly detection
      - id: anomaly_detection
        name: "Detect Anomalies"
        type: agent
        agent: "anomaly_detection_agent"
        parameters:
          data_file: "${transformed_data_path}"
          detection_config: "${processing_config.anomaly_config}"
        outputs:
          anomalies: detected_anomalies
          anomaly_count: anomaly_count
  
  # Step 8: Generate comprehensive report
  - id: generate_report
    name: "Generate Processing Report"
    type: agent
    agent: "report_generation_agent"
    depends_on: ["data_analysis"]
    parameters:
      input_metadata: "${data_metadata}"
      transformation_stats: "${transformation_stats}"
      quality_score: "${data_quality_score}"
      statistical_results: "${statistical_results}"
      anomalies: "${detected_anomalies}"
      chart_files: "${chart_files}"
      output_dir: "${output_directory}"
    outputs:
      report_file: final_report_path
      summary: processing_summary
  
  # Step 9: Archive and cleanup
  - id: archive_results
    name: "Archive Results"
    type: sequential
    depends_on: ["generate_report"]
    tasks:
      # Archive processed data
      - id: archive_data
        name: "Archive Processed Data"
        type: mcp_tool
        tool: "archive_files"
        parameters:
          files: ["${transformed_data_path}", "${final_report_path}"]
          archive_location: "${output_directory}/archive"
          compression: "gzip"
      
      # Clean up temporary files
      - id: cleanup
        name: "Cleanup Temporary Files"
        type: function
        function: "cleanup_temp_files"
        parameters:
          temp_files: ["${cleaned_data_path}", "${features_data_path}"]
      
      # Send completion notification
      - id: notify_completion
        name: "Notify Processing Completion"
        type: mcp_tool
        tool: "send_notification"
        parameters:
          recipients: "${processing_config.notification_recipients}"
          subject: "Data Processing Pipeline Completed"
          message: "Processing completed successfully. Report available at: ${final_report_path}"
          attachments: ["${final_report_path}"]

  # Step 10: Wait period before final cleanup (5 minutes)
  - id: final_wait
    name: "Wait Before Final Cleanup"
    type: delay
    depends_on: ["archive_results"]
    parameters:
      seconds: 300
  
  # Step 11: Final system cleanup
  - id: final_cleanup
    name: "Final System Cleanup"
    type: function
    function: "log"
    depends_on: ["final_wait"]
    parameters:
      message: "Data processing pipeline completed for ${input_file_path}"
      level: "info"

metadata:
  created_by: "Data Engineering Team"
  version: "1.0"
  tags: ["data-processing", "etl", "analytics", "parallel"]
  estimated_duration_minutes: 45
  last_updated: "2024-01-15"